{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import gzip\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_binary_labels_by_digit(labels_in, digit):\n",
    "    d = np.zeros((labels_in.shape[0],1))\n",
    "\n",
    "    for i in range(labels_in.shape[0]):\n",
    "        if labels_in[i] == digit:\n",
    "            d[i] = 1\n",
    "        else:\n",
    "            d[i] = -1\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_subset(features, labels, subset_size):\n",
    "    set_indices = np.arange(labels.shape[0])\n",
    "    subset_indices = np.random.choice(set_indices,5,replace=False)\n",
    "    \n",
    "    subset_features = np.zeros((subset_size, features.shape[1]))\n",
    "    subset_labels = np.zeros((subset_size, 1))\n",
    "    \n",
    "    for i, subset_index in enumerate(subset_indices):\n",
    "        subset_features[i] = features[subset_index]\n",
    "        subset_labels[i] = labels[subset_index]\n",
    "        \n",
    "    return subset_features, subset_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify size of images\n",
    "image_rows = 28\n",
    "image_cols = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open training set image file\n",
    "f = gzip.open('train-images-idx3-ubyte.gz','r')\n",
    "\n",
    "num_train_images = 60000\n",
    "\n",
    "f.read(16)\n",
    "buf = f.read(image_rows * image_cols * num_train_images)\n",
    "train_data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "train_data = train_data.reshape(num_train_images, image_rows, image_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open training set label file\n",
    "\n",
    "f = gzip.open('train-labels-idx1-ubyte.gz','r')\n",
    "f.read(8)\n",
    "\n",
    "buf = f.read(num_train_images)\n",
    "train_labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "train_labels = train_labels.reshape(num_train_images,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (60000, 28, 28, 1) \n",
      "\n",
      "image index 0:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: [5.]\n",
      "(28, 28, 1)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# troubleshoot data import\n",
    "\n",
    "ind = 0\n",
    "\n",
    "print('training data:', train_data.shape, '\\n')\n",
    "\n",
    "# print training image\n",
    "print('image index ' + str(ind) + ':')\n",
    "image = np.asarray(train_data[ind]).squeeze()\n",
    "#plt.title(['image index', ind])\n",
    "plt.imshow(image, cmap='Greys')\n",
    "plt.show()\n",
    "# print training label\n",
    "print('label:', train_labels[ind])\n",
    "\n",
    "print(train_data[ind].shape)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open test set image file\n",
    "f = gzip.open('t10k-images-idx3-ubyte.gz','r')\n",
    "\n",
    "num_test_images = 10000\n",
    "\n",
    "f.read(16)\n",
    "buf = f.read(image_rows * image_cols * num_test_images)\n",
    "test_data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "test_data = test_data.reshape(num_test_images, image_rows, image_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open test set label file\n",
    "\n",
    "f = gzip.open('t10k-labels-idx1-ubyte.gz','r')\n",
    "f.read(8)\n",
    "buf = f.read(num_test_images)\n",
    "test_labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "test_labels = test_labels.reshape(num_test_images,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data: (10000, 28, 28, 1)\n",
      "image index 0:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: [7.]\n"
     ]
    }
   ],
   "source": [
    "# troubleshoot test data import\n",
    "\n",
    "ind = 0\n",
    "\n",
    "print('test data:', test_data.shape)\n",
    "\n",
    "# print training image\n",
    "print('image index ' + str(ind) + ':')\n",
    "image = np.asarray(test_data[ind]).squeeze()\n",
    "#plt.title(['image index', ind])\n",
    "plt.imshow(image, cmap='Greys')\n",
    "plt.show()\n",
    "# print training label\n",
    "print('label:', test_labels[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Sqaures Classification with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamdas:\n",
      " [1.00000000e-04 2.97635144e-04 8.85866790e-04 2.63665090e-03\n",
      " 7.84759970e-03 2.33572147e-02 6.95192796e-02 2.06913808e-01\n",
      " 6.15848211e-01 1.83298071e+00 5.45559478e+00 1.62377674e+01\n",
      " 4.83293024e+01 1.43844989e+02 4.28133240e+02 1.27427499e+03\n",
      " 3.79269019e+03 1.12883789e+04 3.35981829e+04 1.00000000e+05] \n",
      "\n",
      "digit: 0 - [0.0179 0.0179 0.0178 0.0177 0.0177 0.0177 0.0177 0.0175 0.0176 0.0177\n",
      " 0.0176 0.0174 0.0176 0.0173 0.0179 0.0187 0.0198 0.0211 0.0266 0.0416]\n",
      "digit: 1 - [0.0193 0.0193 0.0193 0.0192 0.0192 0.0192 0.0193 0.0192 0.0192 0.0191\n",
      " 0.019  0.019  0.0189 0.0191 0.019  0.0189 0.0184 0.0179 0.0215 0.0458]\n",
      "digit: 2 - [0.0359 0.036  0.036  0.0359 0.0359 0.0359 0.0359 0.0356 0.0356 0.0353\n",
      " 0.0355 0.0357 0.0352 0.0351 0.0353 0.0361 0.0384 0.0433 0.0614 0.0914]\n",
      "digit: 3 - [0.0451 0.0451 0.0451 0.0451 0.045  0.045  0.045  0.045  0.045  0.0447\n",
      " 0.0449 0.0448 0.0446 0.0438 0.044  0.0438 0.0458 0.0511 0.0656 0.0965]\n",
      "digit: 4 - [0.029  0.029  0.029  0.029  0.029  0.029  0.029  0.0289 0.029  0.0291\n",
      " 0.0291 0.0295 0.0297 0.0301 0.0314 0.0337 0.0372 0.0448 0.0706 0.0997]\n",
      "digit: 5 - [0.0537 0.0537 0.0538 0.0538 0.0538 0.0538 0.0538 0.0538 0.0536 0.0536\n",
      " 0.0536 0.0539 0.0537 0.0533 0.0536 0.0557 0.0596 0.074  0.0893 0.0903]\n",
      "digit: 6 - [0.0225 0.0225 0.0225 0.0225 0.0225 0.0224 0.0221 0.0221 0.0221 0.022\n",
      " 0.0222 0.0223 0.022  0.022  0.0213 0.0215 0.0225 0.0254 0.037  0.0864]\n",
      "digit: 7 - [0.031  0.031  0.0311 0.0311 0.0312 0.031  0.0312 0.0311 0.031  0.0308\n",
      " 0.0307 0.0308 0.0311 0.031  0.0299 0.0291 0.0298 0.0338 0.0449 0.0799]\n",
      "digit: 8 - [0.0633 0.0632 0.0632 0.0632 0.0633 0.0632 0.0633 0.0631 0.063  0.0629\n",
      " 0.0629 0.0629 0.0625 0.0627 0.0632 0.0654 0.069  0.0781 0.0918 0.0966]\n",
      "digit: 9 - [0.0567 0.0567 0.0567 0.0567 0.0567 0.0566 0.0566 0.0563 0.0564 0.0565\n",
      " 0.0561 0.0559 0.0559 0.0554 0.0553 0.0575 0.0627 0.0786 0.0945 0.0971]\n",
      "\n",
      "best lamdas:\n",
      " [143.8449888287663, 11288.378916846883, 143.8449888287663, 143.8449888287663, 0.2069138081114788, 143.8449888287663, 428.1332398719387, 1274.2749857031322, 48.32930238571752, 428.1332398719387] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimize best regularizer\n",
    "lamdas = np.logspace(-4,5,num=20)\n",
    "digits = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "print('lamdas:\\n', lamdas, '\\n')\n",
    "\n",
    "# normalize array and reshape into flattened image vectors, and shuffle randomly\n",
    "A_opt = train_data.reshape(num_train_images, image_rows*image_cols)/255\n",
    "d_opt = train_labels\n",
    "\n",
    "# randomly shuffle\n",
    "A_opt = np.hstack((A_opt, d_opt))\n",
    "np.random.shuffle(A_opt)\n",
    "d_opt = A_opt[:,-1:]\n",
    "A_opt = A_opt[:,:-1]\n",
    "\n",
    "# find best lamda for all binary classifiers\n",
    "A_train = A_opt[:50000,:]\n",
    "A_eval = A_opt[50000:,:]\n",
    "best_lamdas = [0] * len(digits)\n",
    "error_rates = np.zeros((len(digits),len(lamdas)))\n",
    "\n",
    "# get error rate for every digit\n",
    "for digit in digits:\n",
    "    \n",
    "    d_train = set_binary_labels_by_digit(d_opt[:50000,:], digit)\n",
    "    d_eval = set_binary_labels_by_digit(d_opt[50000:,:], digit)\n",
    "\n",
    "    # for every lamda, get avg error rate over all digits\n",
    "    for i, lamda in enumerate(lamdas):\n",
    "        \n",
    "        # train with first 50K\n",
    "        w = np.linalg.inv(A_train.T@A_train+lamda*np.identity(A_train.shape[1]))@A_train.T@d_train\n",
    "        \n",
    "        # evaluate classifier with last 10K\n",
    "        d_hat_eval = np.sign(A_eval@w)\n",
    "        errors = np.count_nonzero(d_eval-d_hat_eval!=0)\n",
    "        error_rates[digit, i] = (errors/d_hat_eval.shape[0])\n",
    "    \n",
    "    # pick lamda with lowest error rate\n",
    "    best_lamdas[digit] = lamdas[np.argmin(error_rates[digit])]\n",
    "    print('digit:', digit, '-', error_rates[digit])\n",
    "\n",
    "print('\\nbest lamdas:\\n', best_lamdas, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "\n",
      "training error:\n",
      " [0.0154 0.0176 0.0358 0.0427 0.0284 0.0516 0.0221 0.027  0.0629 0.0578] \n",
      " average: 0.03615\n",
      "\n",
      "test error:\n",
      " [0.0121 0.0161 0.0375 0.0414 0.0299 0.0504 0.0239 0.0283 0.0673 0.0558] \n",
      " average: 0.03627\n"
     ]
    }
   ],
   "source": [
    "# train classifier with optimized regularizer\n",
    "\n",
    "A_train = train_data.reshape(num_train_images, image_rows*image_cols)/255\n",
    "A_test = test_data.reshape(num_test_images, image_rows*image_cols)/255\n",
    "\n",
    "W = np.zeros((A_train.shape[1],len(digits)))\n",
    "print(W.shape)\n",
    "\n",
    "# train and test classifier for every digit\n",
    "train_error_rate_by_digit = [0] * len(digits)\n",
    "test_error_rate_by_digit = [0] * len(digits)\n",
    "\n",
    "for digit in digits:\n",
    "\n",
    "    # train classifier with optimized lamda and training data\n",
    "    d_train = set_binary_labels_by_digit(train_labels, digit)\n",
    "    W[:,digit:digit+1] = np.linalg.inv(A_train.T@A_train+best_lamdas[digit]*np.identity(A_train.shape[1]))@A_train.T@d_train\n",
    "    \n",
    "    # find training error\n",
    "    d_hat_train = np.sign(A_train@W[:,digit:digit+1])\n",
    "    errors = np.count_nonzero(d_train-d_hat_train!=0)\n",
    "    train_error_rate_by_digit[digit] = (errors/d_hat_train.shape[0])\n",
    "    \n",
    "    # test trained classifier with test data\n",
    "    d_test = set_binary_labels_by_digit(test_labels, digit)\n",
    "    d_hat_test = np.sign(A_test@W[:,digit:digit+1])\n",
    "    errors = np.count_nonzero(d_test-d_hat_test!=0)\n",
    "    test_error_rate_by_digit[digit] = (errors/d_hat_test.shape[0])\n",
    "\n",
    "print('\\ntraining error:\\n', np.round(train_error_rate_by_digit,4), '\\n average:', np.round(np.average(train_error_rate_by_digit),5))\n",
    "print('\\ntest error:\\n', np.round(test_error_rate_by_digit,4), '\\n average:', np.round(np.average(test_error_rate_by_digit),5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ista_solve_hot( A, d, la_array ):\n",
    "    # ista_solve_hot: Iterative soft-thresholding for multiple values of\n",
    "    # lambda with hot start for each case - the converged value for the previous\n",
    "    # value of lambda is used as an initial condition for the current lambda.\n",
    "    # this function solves the minimization problem\n",
    "    # Minimize |Ax-d|_2^2 + lambda*|x|_1 (Lasso regression)\n",
    "    # using iterative soft-thresholding.\n",
    "    max_iter = 10**3\n",
    "    tol = 10**(-3)\n",
    "    tau = 1/np.linalg.norm(A,2)**2\n",
    "    n = A.shape[1]\n",
    "    w = np.zeros((n,1))\n",
    "    num_lam = len(la_array)\n",
    "    X = np.zeros((n, num_lam))\n",
    "    for i, each_lambda in enumerate(la_array):\n",
    "        for j in range(max_iter):\n",
    "            z = w - tau*(A.T@(A@w-d))\n",
    "            w_old = w\n",
    "            w = np.sign(z) * np.clip(np.abs(z)-tau*each_lambda/2, 0, np.inf)\n",
    "            X[:, i:i+1] = w\n",
    "            if np.linalg.norm(w - w_old) < tol:\n",
    "                break\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamdas:\n",
      " [1.00000000e-04 2.97635144e-04 8.85866790e-04 2.63665090e-03\n",
      " 7.84759970e-03 2.33572147e-02 6.95192796e-02 2.06913808e-01\n",
      " 6.15848211e-01 1.83298071e+00 5.45559478e+00 1.62377674e+01\n",
      " 4.83293024e+01 1.43844989e+02 4.28133240e+02 1.27427499e+03\n",
      " 3.79269019e+03 1.12883789e+04 3.35981829e+04 1.00000000e+05] \n",
      "\n",
      "digit: 0 - [0.0186 0.0186 0.0186 0.0186 0.0186 0.0186 0.0186 0.0186 0.0187 0.0186\n",
      " 0.0186 0.0186 0.0185 0.0185 0.02   0.0266 0.0374 0.1022 0.116  1.    ]\n",
      "digit: 1 - [0.0187 0.0187 0.0187 0.0187 0.0188 0.0187 0.0188 0.0188 0.0187 0.0187\n",
      " 0.0188 0.0188 0.0188 0.0188 0.0184 0.0202 0.0367 0.1139 0.1189 1.    ]\n",
      "digit: 2 - [0.0378 0.0378 0.0377 0.0376 0.0376 0.0377 0.0375 0.0375 0.0375 0.0375\n",
      " 0.0375 0.0375 0.0375 0.0375 0.0403 0.055  0.0976 0.0984 0.0994 1.    ]\n",
      "digit: 3 - [0.047  0.0469 0.0469 0.0469 0.0469 0.047  0.047  0.0471 0.0471 0.0472\n",
      " 0.0472 0.0472 0.0472 0.0471 0.0485 0.0633 0.0996 0.1    0.1055 1.    ]\n",
      "digit: 4 - [0.0351 0.035  0.035  0.035  0.0351 0.0351 0.0351 0.0352 0.0352 0.0353\n",
      " 0.0353 0.0352 0.0352 0.0352 0.0393 0.0581 0.0953 0.0979 0.116  1.    ]\n",
      "digit: 5 - [0.0616 0.0616 0.0617 0.0615 0.0614 0.0613 0.0612 0.0612 0.0612 0.0611\n",
      " 0.0611 0.0611 0.0611 0.061  0.066  0.0814 0.0884 0.0884 0.0895 1.    ]\n",
      "digit: 6 - [0.0264 0.0264 0.0263 0.0263 0.0263 0.0263 0.0262 0.0262 0.0264 0.0264\n",
      " 0.0265 0.0265 0.0265 0.0265 0.0268 0.035  0.0859 0.0971 0.1021 1.    ]\n",
      "digit: 7 - [0.0293 0.0293 0.0293 0.0294 0.0293 0.0294 0.0293 0.0293 0.0294 0.0294\n",
      " 0.0294 0.0294 0.0295 0.0295 0.031  0.0402 0.0949 0.1065 0.1219 1.    ]\n",
      "digit: 8 - [0.0701 0.0699 0.07   0.0699 0.0697 0.0696 0.0697 0.0698 0.0698 0.0697\n",
      " 0.0696 0.0695 0.0695 0.0695 0.0731 0.0883 0.0971 0.0971 0.0999 1.    ]\n",
      "digit: 9 - [0.07   0.07   0.0698 0.0697 0.0694 0.0694 0.0694 0.0693 0.0692 0.0691\n",
      " 0.069  0.0689 0.0689 0.0688 0.0765 0.0977 0.1007 0.1008 0.1108 1.    ]\n",
      "\n",
      "best lamdas:\n",
      " [48.32930238571752, 428.1332398719387, 0.06951927961775606, 0.00029763514416313193, 0.00029763514416313193, 143.8449888287663, 0.06951927961775606, 0.0001, 16.23776739188721, 143.8449888287663] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimize best regularizer\n",
    "lamdas = np.logspace(-4,5,num=20)\n",
    "digits = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "print('lamdas:\\n', lamdas, '\\n')\n",
    "\n",
    "# normalize array and reshape into flattened image vectors\n",
    "A_opt = train_data.reshape(num_train_images, image_rows*image_cols)/255\n",
    "d_opt = train_labels\n",
    "\n",
    "# randomly shuffle\n",
    "A_opt = np.hstack((A_opt, d_opt))\n",
    "np.random.shuffle(A_opt)\n",
    "d_opt = A_opt[:,-1:]\n",
    "A_opt = A_opt[:,:-1]\n",
    "\n",
    "# find best lamda for all binary classifiers\n",
    "A_train = A_opt[:50000,:]\n",
    "A_eval = A_opt[50000:,:]\n",
    "best_lamdas = [0] * len(digits)\n",
    "   \n",
    "# get error rate for every digit\n",
    "error_rates = np.zeros((len(digits),len(lamdas)))\n",
    "for digit in digits:\n",
    "\n",
    "    # train with first 50K\n",
    "    d_train = set_binary_labels_by_digit(d_opt[:50000,:], digit)\n",
    "    W = ista_solve_hot(A_train,d_train,lamdas)\n",
    "\n",
    "    # evaluate classifier with last 10K\n",
    "    d_eval = set_binary_labels_by_digit(d_opt[50000:,:], digit)\n",
    "    d_hat_eval = np.sign(A_eval@W)\n",
    "    errors = np.count_nonzero(d_eval-d_hat_eval!=0,axis=0)\n",
    "    error_rates[digit] = (errors/d_hat_eval.shape[0])\n",
    "    best_lamdas[digit] = lamdas[np.argmin(error_rates[digit])]\n",
    "    print('digit:', digit, '-', error_rates[digit])\n",
    "\n",
    "print('\\nbest lamdas:\\n', best_lamdas, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training error:\n",
      " [0.019  0.0192 0.0393 0.0466 0.0357 0.0656 0.0235 0.0282 0.071  0.0749] \n",
      " average: 0.0423\n",
      "\n",
      "test error:\n",
      " [0.0157 0.0171 0.0397 0.045  0.0353 0.0632 0.0247 0.028  0.0752 0.071 ] \n",
      " average: 0.04149\n"
     ]
    }
   ],
   "source": [
    "# train classifier with optimized regularizer\n",
    "A_train = train_data.reshape(num_train_images, image_rows*image_cols)/255\n",
    "A_test = test_data.reshape(num_test_images, image_rows*image_cols)/255\n",
    "\n",
    "W = np.zeros((A_train.shape[1],len(digits)))\n",
    "\n",
    "# train and test classifier for every digit\n",
    "train_error_rate_by_digit = [0] * len(digits)\n",
    "test_error_rate_by_digit = [0] * len(digits)\n",
    "\n",
    "for digit in digits:\n",
    "    lamda = [0] * 1\n",
    "    lamda[0] = best_lamdas[digit]\n",
    "    \n",
    "    # train classifier with optimized lamda and training data\n",
    "    d_train = set_binary_labels_by_digit(train_labels, digit)\n",
    "    W[:,digit:digit+1] = ista_solve_hot(A_train,d_train,lamda)\n",
    "    \n",
    "    # find training error\n",
    "    d_hat_train = np.sign(A_train@W[:,digit:digit+1])\n",
    "    errors = np.count_nonzero(d_train-d_hat_train!=0)\n",
    "    train_error_rate_by_digit[digit] = (errors/d_hat_train.shape[0])\n",
    "    \n",
    "    # test trained classifier with test data\n",
    "    d_test = set_binary_labels_by_digit(test_labels, digit)\n",
    "    d_hat_test = np.sign(A_test@W[:,digit:digit+1])\n",
    "    errors = np.count_nonzero(d_test-d_hat_test!=0)\n",
    "    test_error_rate_by_digit[digit] = (errors/d_hat_test.shape[0])\n",
    "\n",
    "print('\\ntraining error:\\n', np.round(train_error_rate_by_digit,4), '\\n average:', np.round(np.average(train_error_rate_by_digit),5))\n",
    "print('\\ntest error:\\n', np.round(test_error_rate_by_digit,4), '\\n average:', np.round(np.average(test_error_rate_by_digit),5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "A_train = train_data.reshape(num_train_images, image_rows*image_cols)/255\n",
    "A_test = test_data.reshape(num_test_images, image_rows*image_cols)/255\n",
    "n_train = np.size(A_train[:,0:1])\n",
    "n_test = np.size(A_test[:,0:1])\n",
    "\n",
    "A_train_1 = np.hstack((A_train, np.ones((n_train,1)) ))\n",
    "A_test_1 = np.hstack((A_test, np.ones((n_test,1)) ))\n",
    "\n",
    "# train and test classifier for every digit\n",
    "train_error_rate_by_digit = [0] * len(digits)\n",
    "test_error_rate_by_digit = [0] * len(digits)\n",
    "digits = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "W = np.zeros((A_train_1.shape[1],len(digits)))\n",
    "#print(W.shape)\n",
    "\n",
    "for digit in digits:\n",
    "\n",
    "   # Train classifier using linear SVM from SK Learn library\n",
    "    clf = LinearSVC(random_state=0, dual=False)\n",
    "    d_train = set_binary_labels_by_digit(train_labels, digit)\n",
    "    clf.fit(A_train_1, np.squeeze(d_train))\n",
    "    W[:,digit:digit+1] = clf.coef_.transpose()\n",
    "    #print('done training', digit)\n",
    "    \n",
    "    # find training error\n",
    "    d_hat_train = np.sign(A_train_1@W[:,digit:digit+1])\n",
    "    errors = np.count_nonzero(d_train-d_hat_train!=0)\n",
    "    train_error_rate_by_digit[digit] = (errors/d_hat_train.shape[0])\n",
    "    \n",
    "    # test trained classifier with test data\n",
    "    d_test = set_binary_labels_by_digit(test_labels, digit)\n",
    "    d_hat_test = np.sign(A_test_1@W[:,digit:digit+1])\n",
    "    errors = np.count_nonzero(d_test-d_hat_test!=0)\n",
    "    test_error_rate_by_digit[digit] = (errors/d_hat_test.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training error:\n",
      " [0.0123 0.0066 0.0238 0.049  0.0161 0.024  0.0211 0.0132 0.1994 0.0728] \n",
      " average: 0.04383\n",
      "\n",
      "test error:\n",
      " [0.0143 0.0067 0.0227 0.0482 0.0186 0.0234 0.023  0.0171 0.207  0.0736] \n",
      " average: 0.04546\n"
     ]
    }
   ],
   "source": [
    "print('\\ntraining error:\\n', np.round(train_error_rate_by_digit,4), '\\n average:', np.round(np.average(train_error_rate_by_digit),5))\n",
    "print('\\ntest error:\\n', np.round(test_error_rate_by_digit,4), '\\n average:', np.round(np.average(test_error_rate_by_digit),5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
